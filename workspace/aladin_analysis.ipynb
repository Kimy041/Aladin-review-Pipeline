{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43589869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hadoop/miniconda3/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3cec7a2d-0f95-42ff-a8d6-9be49f130984;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 124ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3cec7a2d-0f95-42ff-a8d6-9be49f130984\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/8ms)\n",
      "25/07/06 14:49:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AladinReview\") \\\n",
    "    .master(\"spark://namenode:7077\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://client:27017\") \\\n",
    "    .config(\"spark.mongodb.read.database\", \"bookdb\") \\\n",
    "    .config(\"spark.mongodb.read.collection\", \"reviews\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c496891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://client:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://namenode:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AladinReview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xf85dcf456d50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://client:27017/\") \\\n",
    "    .option(\"database\", \"bookdb\") \\\n",
    "    .option(\"collection\", \"reviews\") \\\n",
    "    .load()\n",
    "\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f79486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/06 14:50:52 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/07/06 14:50:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"_id\", StructType([\n",
    "        StructField(\"oid\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"book_id\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"likes\", IntegerType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"review_type\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True)\n",
    "])\n",
    "\n",
    "reviews_df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://client:27017/\") \\\n",
    "    .option(\"database\", \"bookdb\") \\\n",
    "    .option(\"collection\", \"reviews\") \\\n",
    "    .schema(schema) \\\n",
    "    .load() \\\n",
    "    .drop(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1aa64a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+----------+-----+------+-------------+------+------+\n",
      "|  book_id|                         content|      date|likes|rating|  review_type| title|  user|\n",
      "+---------+--------------------------------+----------+-----+------+-------------+------+------+\n",
      "|361016676|일곱 작품 모두 몰입의 대 환장...|2025-04-09|   30|     5|CommentReview|혼모노|은하수|\n",
      "|361016676|분명 각 작품은 지루하지 않게 ...|2025-05-25|   28|     5|CommentReview|혼모노|초록비|\n",
      "+---------+--------------------------------+----------+-----+------+-------------+------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/06 14:49:01 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/07/06 14:49:01 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7de076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 초기 DataFrame ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+-----------------+\n",
      "|name   |age|city    |hobbies          |\n",
      "+-------+---+--------+-----------------+\n",
      "|Alice  |34 |New York|[reading, hiking]|\n",
      "|Bob    |45 |Chicago |[music, movies]  |\n",
      "|Charlie|29 |New York|[hiking, cooking]|\n",
      "|David  |52 |Chicago |[reading, travel]|\n",
      "|Alice  |34 |New York|[reading, hiking]|\n",
      "+-------+---+--------+-----------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType, Row\n",
    "from pyspark.sql.functions import col, lit, upper, count, avg, sum, min, max, expr, countDistinct\n",
    "\n",
    "# SparkSession 생성 (pyspark 셸에서는 'spark' 변수로 자동 생성됨)\n",
    "# spark = SparkSession.builder.appName(\"DataFrameAPIExamples\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# 예제용 데이터 및 스키마\n",
    "data = [(\"Alice\", 34, \"New York\", [\"reading\", \"hiking\"]),\n",
    "        (\"Bob\", 45, \"Chicago\", [\"music\", \"movies\"]),\n",
    "        (\"Charlie\", 29, \"New York\", [\"hiking\", \"cooking\"]),\n",
    "        (\"David\", 52, \"Chicago\", [\"reading\", \"travel\"]),\n",
    "        (\"Alice\", 34, \"New York\", [\"reading\", \"hiking\"])] # 중복 행 추가\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"hobbies\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# 기본 DataFrame 생성\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "print(\"--- 초기 DataFrame ---\")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------+------------+\n",
      "|book_id  |title                   |review_count|\n",
      "+---------+------------------------+------------+\n",
      "|40869703 |소년이 온다             |373         |\n",
      "|362863268|청춘의 독서 (특별증보판)|166         |\n",
      "|25843736 |모순                    |61          |\n",
      "|291370219|채식주의자 (리마스터판) |50          |\n",
      "|361141292|단 한 번의 삶           |35          |\n",
      "|362922298|빛과 실                 |34          |\n",
      "|307692409|급류                    |27          |\n",
      "|170482558|이기적 유전자           |23          |\n",
      "|364959496|네가 있어서             |22          |\n",
      "|365665217|안녕이라 그랬어         |22          |\n",
      "+---------+------------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "top_review_counts = df.groupBy(\"book_id\", \"title\").agg(\n",
    "    count(\"*\").alias(\"review_count\")\n",
    ").orderBy(\"review_count\", ascending=False)\n",
    "\n",
    "top_review_counts.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf01eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------------------------------------------+------------+----------+-----------+\n",
      "|book_id  |title                                                                 |review_count|avg_rating|total_likes|\n",
      "+---------+----------------------------------------------------------------------+------------+----------+-----------+\n",
      "|363870592|열네 살, 한국에 왔어요                                                |3           |5.0       |3          |\n",
      "|353981757|2025 큰별쌤 최태성의 별★별한국사 한국사능력검정시험 심화(1, 2, 3급) 상|3           |5.0       |6          |\n",
      "|330109271|ETS 토익 정기시험 기출문제집 1000 Vol. 4 Listening (리스닝)           |6           |5.0       |16         |\n",
      "|366791947|아무튼, 리코더                                                        |1           |5.0       |0          |\n",
      "|366270620|고전이 답했다 마땅히 가져야 할 부에 대하여                            |16          |5.0       |1          |\n",
      "|365713027|저속노화 마인드셋                                                     |2           |5.0       |0          |\n",
      "|366331655|원피스 111                                                            |1           |5.0       |0          |\n",
      "|365719368|소설 보다 : 여름 2025                                                 |1           |5.0       |1          |\n",
      "|365990683|궤도                                                                  |4           |5.0       |32         |\n",
      "|366174798|미지의 서울 세트 - 전2권                                              |1           |5.0       |8          |\n",
      "+---------+----------------------------------------------------------------------+------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "popular_books = df.groupBy(\"book_id\", \"title\").agg(\n",
    "    count(\"*\").alias(\"review_count\"),\n",
    "    avg(\"rating\").alias(\"avg_rating\"),\n",
    "    sum(\"likes\").alias(\"total_likes\")\n",
    ").orderBy(\"avg_rating\", ascending=False)\n",
    "\n",
    "popular_books.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148bd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+\n",
      "|title |review_date|daily_reviews|\n",
      "+------+-----------+-------------+\n",
      "|혼모노|2025-03-27 |1            |\n",
      "|혼모노|2025-03-29 |1            |\n",
      "|혼모노|2025-04-04 |1            |\n",
      "|혼모노|2025-04-06 |2            |\n",
      "|혼모노|2025-04-07 |1            |\n",
      "|혼모노|2025-04-09 |1            |\n",
      "|혼모노|2025-04-14 |2            |\n",
      "|혼모노|2025-04-28 |1            |\n",
      "|혼모노|2025-04-30 |1            |\n",
      "|혼모노|2025-05-16 |1            |\n",
      "|혼모노|2025-05-22 |1            |\n",
      "|혼모노|2025-05-25 |1            |\n",
      "|혼모노|2025-05-28 |1            |\n",
      "|혼모노|2025-06-14 |1            |\n",
      "|혼모노|2025-07-01 |1            |\n",
      "|혼모노|2025-07-02 |2            |\n",
      "+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, count, col\n",
    "\n",
    "# 날짜 형식 변환\n",
    "reviews_df = df.withColumn(\"review_date\", to_date(\"date\"))\n",
    "\n",
    "# 특정 도서만 필터링\n",
    "target_book_title = \"혼모노\"  # 원하는 책 이름 입력\n",
    "\n",
    "trend = reviews_df.groupBy(\"title\", \"review_date\") \\\n",
    "    .agg(count(\"*\").alias(\"daily_reviews\")) \\\n",
    "    .orderBy(\"review_date\")\n",
    "\n",
    "# 특정 도서 필터링해서 보기\n",
    "trend.filter(col(\"title\").isin(\"혼모노\", \"불변의 법칙\")) \\\n",
    "    .orderBy(\"title\", \"review_date\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44006753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------+-----+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|title                  |user       |likes|rating|content                                                                                                                                                                                                                                                                               |\n",
      "+-----------------------+-----------+-----+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|소년이 온다            |보슬비     |239  |5     |어떻게 얻은 민주주의인데... 우리의 힘은 ‘잊지말기‘, ‘기억하기‘ 그래서 ‘지켜내기‘                                                                                                                                                                                                      |\n",
      "|이기적 유전자          |ikulun     |217  |4     |요즘 알라딘배송이 좀,, 책이 접혀오고, 그냥 읽지만 신경좀 써주세요~한두권도 아닌데…                                                                                                                                                                                                    |\n",
      "|모순                   |시원해     |181  |1     |가정폭력 가해자 남성을 정당화시키고 되려 안쓰러운 존재로 그려낸다. 그 옆에서 평생 폭행당하고 혼자 전 가족을 경제적으로 책임져야만 했던 여성의 고통은 행복과 활력이라고 묘사한다. 가해자와 피해자의 위치를 바꾼다.  상대방이 필름 끊긴 상태에서 성관계하면 강간인데 강간을 미화한다.   |\n",
      "|소년이 온다            |책읽는나무 |149  |5     |아픔과 슬픔은 경험해봐야 크게  공감할 수 있다지만, 글을 읽으면서도 이미 눈은 슬프고, 가슴은 아프다.무섭도록 고요했었던 그 도시, 오월의 봄.감히 가늠할 수 없는 고통스런 학살을,오랫동안 기억할 것이다.인간의 존엄성과 함께!                                                            |\n",
      "|소년이 온다            |코코몽     |130  |5     |역사를 잊은 민족에게 미래는 없다.                                                                                                                                                                                                                                                     |\n",
      "|채식주의자 (리마스터판)|그것은실로 |125  |2     |뒤늦게 읽었고 여러모로 놀랐다. 작가적 역량이 부족한 것은 문제가 되지 않지만, 역량이 부족한데 선동적인 것은 문제적이다.                                                                                                                                                                |\n",
      "|모순                   |comykim    |120  |5     |가해자 남성을 정당화..글쎄요 그래서 제목이 ‘모순‘아닐까요? 제목에서 이미 끝났다고 생각합니다..                                                                                                                                                                                        |\n",
      "|소년이 온다            |the        |117  |5     |출근길에 지하철에서 숨죽여 오열하며 마지막 장을 다 읽었습니다. 고통 속에서 한 자 한 자 적어 나갔을 작가님을 위로하고 싶어집니다.                                                                                                                                                      |\n",
      "|소년이 온다            |appletreeje|109  |5     |습자지에 핏망울이 꽃처럼 스며드는 이야기. 새의 영혼같이 나직나직 들려주는 이 책을, 한번에 읽지 못했다. 조금씩 조금씩 읽어가며..겪어내지 못한 일에 대한 공분이나 애도는 얼마나 가벼운가, ˝대신 잘 써주셔야 합니다.제대로 써야 합니다. 아무도 내 동생을 더이상 모독할 수 없도록 써주세요|\n",
      "|모순                   |그런거없다 |109  |4     |작중 엄마와 딸을 이토록 불행하게 그렸는데 어떻게 가정폭력을 미화했다는 평이 가장 위에 올라와 있는 걸까.. 소설에 몰입을 한 게 맞는가.. 그들에게 주목받지 못한 안진진에게 안타까움마저 든다. 극중 인물의 특정 행위에만 꽂혀 맥락과 서사를 외면하는 건 주관이 아니라 오독이다.           |\n",
      "+-----------------------+-----------+-----+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_reviews = df.select(\"title\", \"user\", \"likes\", \"rating\", \"content\") \\\n",
    "    .orderBy(\"likes\", ascending=False)\n",
    "\n",
    "top_reviews.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2b219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
